---
title: "Multivariate Statistical Analysis - Notes"
output:
  html_notebook: default
  pdf_document: default
runtime: shiny
---


# Introduction / Lecture 1

### The organisation of Data



### Descriptive statistics
* The "meaning" of the sample mean, is in fact the *center of data* 
  +  In R: colMeans()
* 1/n or 1/(n-1) also refered to as a __*sample convariance*__  
  +  In R: cmd cov()
* For *k* = 1, 2 ...p is refered to as *skk* as the __*sample variance*__ for attributes

#### Sample covariance matrix
Given the data matrix __*X*__ 
* Its sample correlation matrix is a (*p x p*)-matrix
* For each *i, k = 1, 2,...p,* there is 
  + $\frac{S_{ik}}{\sqrt s_{ii} \sqrt s_{kk}}$
* For each *i, k = 1, 2,...p,* there is $-1\le r_{ik} \le 1$

## Statistical distance
Given data as an *(n x p)*-matrix and any two locations $${\mathbf{x} = \left(\begin{array}{rrr}x1 \\ x2\\ ... \\ x_p \end{array}\right)}$$ and 
$${\mathbf{y} = \left(\begin{array}{rrr}y_1 \\ y_2\\ ... \\ y_p \end{array}\right)}$$
The statistical distance ( __Mahalanobis distance__ ) between *x* and *y* is: 
  $$d(x, y)= {\sqrt (x-y)S^{-1}_{n}(x-y)}$$
This can be computed with
* mahalanobis()
  + This example is of why column is used in AMSA.
  + This is needed in relation of preparing the distance between $\mu_1$ and $\mu_0$ in regards of hypothesis test of the mean for a single random variable $|\mu_1 - \mu_0|$

If we have two locations   $${\mathbf{x} = \left(\begin{array}{rrr}x1 \\ x2\end{array}\right)}$$ and  
$${\mathbf{y} = \left(\begin{array}{rrr}y_1 \\ y_2\end{array}\right)}$$ we know the straight-line distance (Euclidian distance (see algorithm))
* But that distance doesn't take statistical variablities of data into account - relatively greater varuavukutues shouldn't be accounted for queally to a small variabilities. 

Given $p = 2$ and sample convariance matrix $$S_n = (\begin{array}{rrr}
s_{11} & 0  \\ 0 & s_{22}\end{array})$$ then
$$ d(x,y) = \sqrt(x-y)' * S^{-1}_{n} * (x-y) = \sqrt (\frac{x_1 - y_2}{s_{11}})^2 + (\frac {x_{2} - y_{2}}{s_{22}})^2$$
# Lecture 2

### Random Matrix
* A random vector is a special case of a random matrix. 
  + Example: 
${\mathbf{r} = \left(\begin{array}{rrr}X_{11} \\ X_{21} \\ ... \\ X_{j1} \\ ...\\ X_{n1} \end{array}\right)}$ or $(X_{11}, X_{21}, ..., X_{j1}, ..., X_{n1})^T$
  
* General forms of the random matrix and its expectation matrix [See Slides Lecture2, slide 8]
  + Note: Each row of the random matrix satisfies the same joint distribution as vector $(X_1, X_2, ..., X_p)'$

* In that case, the properties of a random matrix is determined by the random vector 
$X = (X_1, X_2, ..., X_p)'$

* Each row of $E(X)$ is $(E(X_1) = \mu_1, E(X_2) = \mu_2, ...., E(X_p) = \mu_p)$ denoted by ${\mathbf{\mu} = \left(\begin{array}{rrr}\mu_1\\ \mu_2 \\ ... \\ \mu_p \end{array}\right)}$ and referred to as (*population*) mean vector.


### Population covariance matrix

Let $X = (X_1, X_2, ..., X_p)'$ is denoted by $\Sigma$ or Cov(X).

* $Cov(X_1, X_2) ? E[(X_1 - \mu_1)(X_2 - \mu_2)]$ therefore $Cov(X_1, X_2) = Cov(X_2, X_1)$ and thus $\Sigma$ is symmetric

* Useful results: 
  + $Cov(X_j, X_j) = Var(X_j)$
  + $Cov(X_i, X_i) = 0$ does **not** indicate two random variables are independent. 
  + $Cov(X_i, X_j) = E(X_i, X_j) - E(X_i)E(X_j)$
  + $Cov(X) = E[(X - \mu) * (X - \mu)']$
  
  
### Population correlation coefficient matrix (p x p)
* Let $\sigma_{jk} = Cov(X_j, X_k)$ then the correlation coefficient between $X_j$ and $X_k$ is $$ \rho_{jk} = \frac{\sigma_{jk}}{\sqrt\sigma_{jj}*\sigma_{kk}}$$

*The population correlation coefficient matrix is a symmetric matrix. 


### Population covariance matrix (pxp) and linear transformations

* Let $C$ be a nummeric (q x p) matrix, then we have: 
  + $E(C * X) = C * \mu$ and $Cov(C * X) = C * \Sigma * C'$.
  
# Lecture 3

## Notations

* Having a *Random Matrix* ($X_1, X_2, ...., X_p$) and *Data Matrix* ($x_{11}, x_{12}, ...., x_{1k}, ..., x_{1p}$)
  + Note: $X_1, X_2,...,X_n$ can be view as a random sample of $X$
  + The random sample could be used to construct estimators for inferring the statistical properties of the joint distribution of $X$, e.g. $\mu$ and $\Sigma$
  + Via realisation estimation is completed of this random sample (i.e. the data matrix)

### Three descriptive statistics / estimators
* Sample mean (vector) $\overline{X} = \frac{1}{n}\sum_{j = 1}^{n}X_j$ [Estimator of $\mu$]

* Sample of covariance matrix $S_n = \frac{1}{n}{\sum_{j = 1}^{n}} {(\bar{X_j}-{\overline {X})({X_j}-\overline {X})}}$ is $= \frac{1}{n}\sum_{j =1}^{n} X_j X_j^Â´ - \overline{X} * \overline{X}'$

* (Unbiassed) sample covariance matric [Estimator of $\Sigma$]
$S = {(\frac{n}{n-1}) * S_n = \frac{1}{n-1}\sum_{j = 1}^{n} (X_j \overline{X})(X_j-\overline{X})'}$









---
title: "Multivariate Statistical Analysis - Notes"
output:
  html_notebook: default
  pdf_document: default
runtime: shiny
---


# Introduction / Lecture 1

### The organisation of Data



### Descriptive statistics
* The "meaning" of the sample mean, is in fact the *center of data* 
  +  In R: colMeans()
* 1/n or 1/(n-1) also refered to as a __*sample convariance*__  
  +  In R: cmd cov()
* For *k* = 1, 2 ...p is refered to as *skk* as the __*sample variance*__ for attributes

#### Sample covariance matrix
Given the data matrix __*X*__ 
* Its sample correlation matrix is a (*p x p*)-matrix
* For each *i, k = 1, 2,...p,* there is 
  + $\frac{S_{ik}}{\sqrt s_{ii} \sqrt s_{kk}}$
* For each *i, k = 1, 2,...p,* there is $-1\le r_{ik} \le 1$

## Statistical distance
Given data as an *(n x p)*-matrix and any two locations $${\mathbf{x} = \left(\begin{array}{rrr}x1 \\ x2\\ ... \\ x_p \end{array}\right)}$$ and 
$${\mathbf{y} = \left(\begin{array}{rrr}y_1 \\ y_2\\ ... \\ y_p \end{array}\right)}$$
The statistical distance ( __Mahalanobis distance__ ) between *x* and *y* is: 
  $$d(x, y)= {\sqrt (x-y)S^{-1}_{n}(x-y)}$$
This can be computed with
* mahalanobis()
  + This example is of why column is used in AMSA.
  + This is needed in relation of preparing the distance between $\mu_1$ and $\mu_0$ in regards of hypothesis test of the mean for a single random variable $|\mu_1 - \mu_0|$

If we have two locations   $${\mathbf{x} = \left(\begin{array}{rrr}x1 \\ x2\end{array}\right)}$$ and  
$${\mathbf{y} = \left(\begin{array}{rrr}y_1 \\ y_2\end{array}\right)}$$ we know the straight-line distance (Euclidian distance (see algorithm))
* But that distance doesn't take statistical variablities of data into account - relatively greater varuavukutues shouldn't be accounted for queally to a small variabilities. 

Given $p = 2$ and sample convariance matrix $$S_n = (\begin{array}{rrr}
s_{11} & 0  \\ 0 & s_{22}\end{array})$$ then
$$ d(x,y) = \sqrt(x-y)' * S^{-1}_{n} * (x-y) = \sqrt (\frac{x_1 - y_2}{s_{11}})^2 + (\frac {x_{2} - y_{2}}{s_{22}})^2$$
# Lecture 2

### Random Matrix
* A random vector is a special case of a random matrix. 
  + Example: 
${\mathbf{r} = \left(\begin{array}{rrr}X_{11} \\ X_{21} \\ ... \\ X_{j1} \\ ...\\ X_{n1} \end{array}\right)}$ or $(X_{11}, X_{21}, ..., X_{j1}, ..., X_{n1})^T$
  
* General forms of the random matrix and its expectation matrix [See Slides Lecture2, slide 8]
  + Note: Each row of the random matrix satisfies the same joint distribution as vector $(X_1, X_2, ..., X_p)'$

* In that case, the properties of a random matrix is determined by the random vector 
$X = (X_1, X_2, ..., X_p)'$

* Each row of $E(X)$ is $(E(X_1) = \mu_1, E(X_2) = \mu_2, ...., E(X_p) = \mu_p)$ denoted by ${\mathbf{\mu} = \left(\begin{array}{rrr}\mu_1\\ \mu_2 \\ ... \\ \mu_p \end{array}\right)}$ and referred to as (*population*) mean vector.


### Population covariance matrix

Let $X = (X_1, X_2, ..., X_p)'$ is denoted by $\Sigma$ or Cov(X).

* $Cov(X_1, X_2) ? E[(X_1 - \mu_1)(X_2 - \mu_2)]$ therefore $Cov(X_1, X_2) = Cov(X_2, X_1)$ and thus $\Sigma$ is symmetric

* Useful results: 
  + $Cov(X_j, X_j) = Var(X_j)$
  + $Cov(X_i, X_i) = 0$ does **not** indicate two random variables are independent. 
  + $Cov(X_i, X_j) = E(X_i, X_j) - E(X_i)E(X_j)$
  + $Cov(X) = E[(X - \mu) * (X - \mu)']$
  
  
### Population correlation coefficient matrix (p x p)
* Let $\sigma_{jk} = Cov(X_j, X_k)$ then the correlation coefficient between $X_j$ and $X_k$ is $$ \rho_{jk} = \frac{\sigma_{jk}}{\sqrt\sigma_{jj}*\sigma_{kk}}$$

*The population correlation coefficient matrix is a symmetric matrix. 


### Population covariance matrix (pxp) and linear transformations

* Let $C$ be a nummeric (q x p) matrix, then we have: 
   $$E(C * X) = C * \mu$ and $Cov(C * X) = C * \Sigma * C'$$.
  
# Lecture 3

## Notations

* Having a *Random Matrix* ($X_1, X_2, ...., X_p$) and *Data Matrix* ($x_{11}, x_{12}, ...., x_{1k}, ..., x_{1p}$)
  + Note: $X_1, X_2,...,X_n$ can be view as a random sample of $X$
  + The random sample could be used to construct estimators for inferring the statistical properties of the joint distribution of $X$, e.g. $\mu$ and $\Sigma$
  + Via realisation estimation is completed of this random sample (i.e. the data matrix)

### Three descriptive statistics / estimators
* Sample mean (vector) $\overline{X} = \frac{1}{n}\sum_{j = 1}^{n}X_j$ [Estimator of $\mu$]

* Sample of covariance matrix $$S_n = \frac{1}{n}{\sum_{j = 1}^{n}} {(\bar{X_j}-{\overline {X})({X_j}-\overline {X})}}$$ is $$= \frac{1}{n}\sum_{j =1}^{n} X_j X_j^Â´ - \overline{X} * \overline{X}'$$

* (Unbiassed) sample covariance matric [Estimator of $\Sigma$]
$$S = {(\frac{n}{n-1}) * S_n = \frac{1}{n-1}\sum_{j = 1}^{n} (X_j \overline{X})(X_j-\overline{X})'}$$


# Lecture 4 

### The univariate normal distribution 

* Assume a r.v. *X* satisfies a normal / Gaussian distribution
  + Probability density function $f(x) = \frac{1}{\sqrt 2 \pi\sigma ^2}exp \{ -\frac{1}{2} (\frac{x - \mu}{\sigma})^2\}$
  + $E(X) = \mu$ and $Var(X) = \sigma^2$
* Estimation about $\mu$ and $\sigma$ with some given data: 
  + confidence interval and hypothesis test
* Assumption: the underlying distribution of the given data is normal distribution.
  + How to access the normality of my data? 
  + Univariate normality can be detected by first drawing a QQ-plot (In R: cmd qqnorm)
  + Follow it up with a hypothesis test for linearity using correlation coefficient $r_Q$. 

* $H_0$: data is normally distributed
* Rejection regions is $r_Q$ < critical value for a given significan level of test. 
* Sample correlation computed on normal quantile plot: 
  + (left) $r_Q = 1.03$ and
  + (right) $r_Q = 0.93$ with $n = 42$ Obsrve that critical value of a 5% significance
  + left test: $0.9726 (n = 40), 0.9749 (n = 45)$ so $r_Q$ is the rejection region, so the $H_0$ is rejected. 
* In R: 
`df <- read.table("./Desktop/SDU/Multi Statistik/MultiStatisticNotes/T4-1-DAT", header = F)` `test <- qqnorm(df[,1], pch = 1, frame = FALSE)` `rQ <- cor(test$x, test$y)` 
* Not all data are normally distributed. Particularly true for categorical data e.g. gender distribution of a population.  

#### Multivariate normal distribution
* Considering $X = (X_1, X_2, ..., X_p)'$ and $x = (x_1, x_2, ..., x_p)'$

$$ f(x) = \frac{1}{(2\pi)^{p/2}}  exp\{-(x- \mu)'\Sigma^{-1}(x - \mu)/2\} $$
* Where $\Sigma$ is positive definite. If the above equation is satisfied, we write $X \sim N_p(\mu, \Sigma)$ and say $X$ satisfies $p-dim$ distribution with $\mu$ and $\Sigma$

* $E(X) = \mu$ $Cov(X) = \Sigma$


#### Property #2

* Assume $X \sim N_p(\mu, \Sigma)$ we have $(X - \mu)'\Sigma^{-1}(X-\mu)\sim x_{p}^{2}(\alpha)$ has probability $1-\alpha$

$$\begin{array}{rrr} Probability & Obsserved count & Expected count \\
0.25 & 17 & 10.5 \\
0.50 & 29 & 21 \\
0.75 & 33 & 31.5
\end{array}$$
* Expected number of observations versus data. *Note* $n = 42$



* Assume $X \sim N_p(\mu, \Sigma)$ and $a' = (a_1, a_2, ..., a_p)$ then we have 
  + $a'X \sim N_p(a'\mu, a'\Sigma a)$
  + AND in particular we have all marginal $X_i \sim N(\mu_i, \sigma_{ii})$



















